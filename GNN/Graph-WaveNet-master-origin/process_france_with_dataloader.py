import pandas as pd
import numpy as np
import os
import argparse
from datetime import datetime
import pickle

# Import the Dataset_Opennem class from dataloader.py
from dataloader import Dataset_Opennem

def prepare_france_csv_for_dataloader():
    """
    Prepare the new France CSV data (which already has timestamps) for Dataset_Opennem
    """
    print("ğŸ”„ å‡†å¤‡æ–°çš„France CSVæ•°æ®ï¼ˆå·²åŒ…å«æ—¶é—´æˆ³ï¼‰...")
    
    # Read the CSV file that already has timestamps and proper structure
    csv_file = 'data/France_processed_0.csv'
    df = pd.read_csv(csv_file)
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"åˆ—å: {list(df.columns)}")
    
    # Verify the time column
    print(f"æ—¶é—´åˆ—: {df['date'].head()}")
    print(f"æ—¶é—´èŒƒå›´: {df['date'].iloc[0]} åˆ° {df['date'].iloc[-1]}")
    
    # The data is already in the correct format for Dataset_Opennem
    # Just need to specify target column (use the last power generation column)
    feature_columns = [col for col in df.columns if col != 'date']
    target_col = feature_columns[-1]  # Use last column as target
    
    print(f"ç‰¹å¾åˆ—æ•°é‡: {len(feature_columns)}")
    print(f"ç›®æ ‡åˆ—: {target_col}")
    
    # Save the data (it's already in the right format)
    output_file = 'data/france_for_dataloader.csv'
    df.to_csv(output_file, index=False)
    print(f"âœ… æ•°æ®ä¿å­˜åˆ°: {output_file}")
    
    # Check data frequency
    time_diff = pd.to_datetime(df['date'].iloc[1]) - pd.to_datetime(df['date'].iloc[0])
    freq_detected = pd.infer_freq(pd.to_datetime(df['date'][:100]))
    print(f"æ£€æµ‹åˆ°çš„æ•°æ®é¢‘ç‡: {freq_detected} (æ—¶é—´é—´éš”: {time_diff})")
    
    return output_file, target_col, len(feature_columns)

def create_france_dataset_using_dataloader(data_file, target_col, seq_length=12, pred_length=12):
    """
    Use Dataset_Opennem to create France dataset with configurable sequence lengths
    """
    print(f"ğŸ“Š ä½¿ç”¨Dataset_Opennemåˆ›å»ºFranceæ•°æ®é›† (seq_length={seq_length}, pred_length={pred_length})...")
    
    # Create datasets using Dataset_Opennem
    # Parameters for time series (configurable based on input)
    size = [seq_length, 0, pred_length]  # [seq_len, label_len, pred_len]
    
    datasets = {}
    for flag in ['train', 'val', 'test']:
        print(f"åˆ›å»º {flag} æ•°æ®é›†...")
        dataset = Dataset_Opennem(
            root_path='data',
            flag=flag,
            size=size,
            features='M',  # Use all features (multivariate)
            data_path='france_for_dataloader.csv',
            target=target_col,
            scale=True,
            timeenc=1,  # Use advanced time encoding from dataloader
            freq='h'  # Hourly frequency (detected from the data)
        )
        datasets[flag] = dataset
        print(f"  {flag} æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        # Get a sample to check dimensions
        if len(dataset) > 0:
            sample = dataset[0]
            print(f"  æ ·æœ¬å½¢çŠ¶: X={sample[0].shape}, Y={sample[1].shape}, X_mark={sample[2].shape}, Y_mark={sample[3].shape}")
    
    return datasets

def convert_to_graph_wavenet_format(datasets):
    """
    Convert Dataset_Opennem output to Graph WaveNet format
    """
    print("ğŸ”„ è½¬æ¢ä¸ºGraph WaveNetæ ¼å¼...")
    
    output_dir = 'data/FRANCE'
    os.makedirs(output_dir, exist_ok=True)
    
    # First pass: collect all data to find global min/max for scaling
    all_X = []
    all_Y = []
    
    for flag in ['train', 'val', 'test']:
        dataset = datasets[flag]
        if len(dataset) == 0:
            continue
            
        # Collect all samples for this split
        X_list, Y_list = [], []
        
        for i in range(len(dataset)):
            seq_x, seq_y, seq_x_mark, seq_y_mark = dataset[i]
            X_list.append(seq_x)
            Y_list.append(seq_y)
        
        if X_list:
            X = np.stack(X_list, axis=0)
            Y = np.stack(Y_list, axis=0)
            all_X.append(X)
            all_Y.append(Y)
    
    # Find global min and max for scaling to 0-1 range
    if all_X and all_Y:
        global_X = np.concatenate(all_X, axis=0)
        global_Y = np.concatenate(all_Y, axis=0)
        all_data = np.concatenate([global_X.flatten(), global_Y.flatten()])
        
        data_min = all_data.min()
        data_max = all_data.max()
        data_range = data_max - data_min
        
        print(f"ğŸ“Š å…¨å±€æ•°æ®ç»Ÿè®¡:")
        print(f"  åŸå§‹èŒƒå›´: {data_min:.4f} - {data_max:.4f}")
        print(f"  å°†ç¼©æ”¾åˆ°: 0.0 - 1.0")
    
    # Second pass: process and save each split with scaling
    for flag in ['train', 'val', 'test']:
        dataset = datasets[flag]
        if len(dataset) == 0:
            continue
            
        print(f"å¤„ç† {flag} æ•°æ®é›†...")
        
        # Collect all samples
        X_list, Y_list = [], []
        X_mark_list, Y_mark_list = [], []
        
        for i in range(len(dataset)):
            seq_x, seq_y, seq_x_mark, seq_y_mark = dataset[i]
            X_list.append(seq_x)
            Y_list.append(seq_y)
            X_mark_list.append(seq_x_mark)
            Y_mark_list.append(seq_y_mark)
        
        # Stack all samples
        X = np.stack(X_list, axis=0)  # Shape: (num_samples, seq_len, num_features)
        Y = np.stack(Y_list, axis=0)  # Shape: (num_samples, pred_len, num_features)
        X_mark = np.stack(X_mark_list, axis=0)  # Time features
        Y_mark = np.stack(Y_mark_list, axis=0)
        
        print(f"  åŸå§‹å½¢çŠ¶: X={X.shape}, Y={Y.shape}, X_mark={X_mark.shape}")
        
        # Apply min-max scaling to get 0-1 range
        X_scaled = (X - data_min) / data_range
        Y_scaled = (Y - data_min) / data_range
        
        # Ensure values are strictly in [0, 1] range
        X_scaled = np.clip(X_scaled, 0.0, 1.0)
        Y_scaled = np.clip(Y_scaled, 0.0, 1.0)
        
        print(f"  ç¼©æ”¾åèŒƒå›´: X=[{X_scaled.min():.4f}, {X_scaled.max():.4f}], Y=[{Y_scaled.min():.4f}, {Y_scaled.max():.4f}]")
        
        # For Graph WaveNet, we need to reshape to include node dimension
        # Treat each feature as a separate node
        num_samples, seq_len, num_features = X_scaled.shape
        _, pred_len, _ = Y_scaled.shape
        
        # Reshape X: (num_samples, seq_len, num_nodes, 1) for data + time features
        X_reshaped = X_scaled.reshape(num_samples, seq_len, num_features, 1)
        Y_reshaped = Y_scaled.reshape(num_samples, pred_len, num_features, 1)
        
        # Add time features as second channel
        # X_mark shape: (num_samples, seq_len, time_features)
        # We need to broadcast it to (num_samples, seq_len, num_nodes, time_features)
        time_features_expanded = np.broadcast_to(
            X_mark[:, :, np.newaxis, :], 
            (num_samples, seq_len, num_features, X_mark.shape[-1])
        )
        
        # Take only the first time feature and scale it to 0-1 range
        time_feature_raw = time_features_expanded[:, :, :, 0:1]
        
        # Scale time features to 0-1 range
        time_min = time_feature_raw.min()
        time_max = time_feature_raw.max()
        time_range = time_max - time_min
        if time_range > 0:
            time_feature_scaled = (time_feature_raw - time_min) / time_range
        else:
            time_feature_scaled = np.zeros_like(time_feature_raw)
        
        # Ensure time features are in [0, 1] range
        time_feature_scaled = np.clip(time_feature_scaled, 0.0, 1.0)
        
        # Concatenate data and time features
        X_final = np.concatenate([X_reshaped, time_feature_scaled], axis=-1)
        Y_final = np.concatenate([Y_reshaped, time_feature_scaled[:, -pred_len:, :, :]], axis=-1)
        
        print(f"  Graph WaveNetæ ¼å¼: X={X_final.shape}, Y={Y_final.shape}")
        
        # Create offsets (required by Graph WaveNet)
        x_offsets = np.arange(-seq_len + 1, 1)
        y_offsets = np.arange(1, pred_len + 1)
        
        # Save in Graph WaveNet format
        np.savez_compressed(
            os.path.join(output_dir, f"{flag}.npz"),
            x=X_final,
            y=Y_final,
            x_offsets=x_offsets.reshape(-1, 1),
            y_offsets=y_offsets.reshape(-1, 1),
        )
        
        print(f"  âœ… {flag} æ•°æ®ä¿å­˜å®Œæˆ: {X_final.shape}")
    
    print(f"âœ… æ‰€æœ‰æ•°æ®ä¿å­˜åˆ°: {output_dir}")
    return num_features

def generate_adjacency_matrix_for_features(num_nodes):
    """
    Generate adjacency matrix for France dataset based on power generation relationships
    """
    print(f"ğŸ”— ä¸º{num_nodes}ä¸ªç”µåŠ›ç‰¹å¾ç”Ÿæˆé‚»æ¥çŸ©é˜µ...")
    
    # Power generation types have natural relationships
    # Renewable sources might be connected, fossil fuels connected, etc.
    adj = np.zeros((num_nodes, num_nodes), dtype=np.float32)
    
    # Create a more realistic connectivity pattern for power generation
    # Based on the order of columns in the France dataset
    power_types = [
        'Biomass', 'Fossil_Gas', 'Fossil_Hard_coal', 'Fossil_Oil',
        'Hydro_Run_of_river', 'Hydro_Water_Reservoir', 'Nuclear',
        'Solar', 'Waste', 'Wind_Onshore'
    ]
    
    # Group related power sources
    renewables = [0, 4, 5, 7, 9]  # Biomass, Hydro Run-of-river, Hydro Water Reservoir, Solar, Wind Onshore
    fossils = [1, 2, 3]  # Fossil Gas, Hard coal, Oil
    hydro = [4, 5]  # Hydro types
    
    # Connect renewable sources
    for i in renewables:
        for j in renewables:
            if i != j:
                adj[i, j] = 1.0
    
    # Connect fossil fuel sources
    for i in fossils:
        for j in fossils:
            if i != j:
                adj[i, j] = 1.0
    
    # Connect hydro sources strongly
    for i in hydro:
        for j in hydro:
            if i != j:
                adj[i, j] = 1.0
    
    # Connect all adjacent power types (temporal/operational relationships)
    for i in range(num_nodes):
        if i > 0:
            adj[i, i-1] = 1.0
        if i < num_nodes - 1:
            adj[i, i+1] = 1.0
    
    # Self connections
    np.fill_diagonal(adj, 1.0)
    
    # Create sensor IDs based on actual power generation types
    sensor_ids = [f"france_{power_types[i] if i < len(power_types) else f'feature_{i}'}" for i in range(num_nodes)]
    sensor_id_to_ind = {sensor_id: i for i, sensor_id in enumerate(sensor_ids)}
    
    # Save adjacency matrix
    adj_dir = 'data/sensor_graph'
    os.makedirs(adj_dir, exist_ok=True)
    
    with open(os.path.join(adj_dir, 'adj_mx_france.pkl'), 'wb') as f:
        pickle.dump([sensor_ids, sensor_id_to_ind, adj.astype(np.float32)], f, protocol=2)
    
    print(f"âœ… é‚»æ¥çŸ©é˜µä¿å­˜åˆ°: {adj_dir}/adj_mx_france.pkl")
    print(f"é‚»æ¥çŸ©é˜µå½¢çŠ¶: {adj.shape}")
    print(f"è¿æ¥æ•°: {(adj > 0).sum()}")
    print(f"ç”µåŠ›ç±»å‹: {sensor_ids}")
    
    return adj

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Process real France power generation data using Dataset_Opennem')
    parser.add_argument('--step', type=str, default='all', 
                       choices=['all', 'prepare', 'process', 'adj'],
                       help='Which step to run: all, prepare, process, or adj')
    parser.add_argument('--seq_length', type=int, default=12,
                       help='Input sequence length')
    parser.add_argument('--pred_length', type=int, default=12,
                       help='Prediction sequence length')
    
    args = parser.parse_args()
    
    print("ğŸ‡«ğŸ‡· çœŸå®Franceç”µåŠ›æ•°æ®é›†å¤„ç†å¼€å§‹ï¼ˆä½¿ç”¨Dataset_Opennemï¼‰...")
    print(f"ğŸ“Š åºåˆ—é•¿åº¦é…ç½®: seq_length={args.seq_length}, pred_length={args.pred_length}")
    
    if args.step in ['all', 'prepare']:
        print("\næ­¥éª¤ 1: å‡†å¤‡CSVæ•°æ®...")
        data_file, target_col, num_features = prepare_france_csv_for_dataloader()
    else:
        data_file = 'data/france_for_dataloader.csv'
        target_col = 'Wind Onshore  - Actual Aggregated [MW]'  # Default target
        num_features = 10  # Default value for France (10 features)
    
    if args.step in ['all', 'process']:
        print("\næ­¥éª¤ 2: ä½¿ç”¨Dataset_Opennemå¤„ç†æ•°æ®...")
        datasets = create_france_dataset_using_dataloader(data_file, target_col, args.seq_length, args.pred_length)
        
        print("\næ­¥éª¤ 3: è½¬æ¢ä¸ºGraph WaveNetæ ¼å¼...")
        num_features = convert_to_graph_wavenet_format(datasets)
        
        print("\nğŸ” éªŒè¯ç”Ÿæˆçš„æ•°æ®:")
        for split in ['train', 'val', 'test']:
            file_path = f'data/FRANCE/{split}.npz'
            if os.path.exists(file_path):
                data = np.load(file_path)
                print(f"{split.upper()} - X: {data['x'].shape}, Y: {data['y'].shape}")
    
    if args.step in ['all', 'adj']:
        print(f"\næ­¥éª¤ 4: ç”Ÿæˆ{num_features}ä¸ªç”µåŠ›ç‰¹å¾çš„é‚»æ¥çŸ©é˜µ...")
        generate_adjacency_matrix_for_features(num_features)
    
    print("\nğŸ‰ çœŸå®Franceç”µåŠ›æ•°æ®é›†å¤„ç†å®Œæˆ!")
    print("ğŸ“Š æ•°æ®ç‰¹ç‚¹:")
    print("  âœ… ä½¿ç”¨çœŸå®æ—¶é—´æˆ³ (2015-2024)")
    print("  âœ… 10ç§ç”µåŠ›ç”Ÿæˆç±»å‹")
    print("  âœ… å°æ—¶çº§æ•°æ®é¢‘ç‡")
    print("  âœ… Dataset_Opennemä¸“ä¸šå¤„ç†")
    print("  âœ… æ•°æ®èŒƒå›´æ§åˆ¶åœ¨ [0, 1]")
    print(f"  âœ… åºåˆ—é•¿åº¦: {args.seq_length} -> {args.pred_length}")
    print("\nå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®­ç»ƒ:")
    print(f"python train.py --data data/FRANCE --gcn_bool --adjtype doubletransition --addaptadj --randomadj --epochs 50 --seq_length {args.seq_length} --pred_length {args.pred_length}")
    
