import os
import torch
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
from datetime import datetime
from models.handler import train, test
import argparse
import pandas as pd
import numpy as np
import json
import csv
from utils.data_utils import load_and_process_dataset, print_dataset_info


#  python main.py --wandb --dataset France_processed_0 --wandb_project 'NEWFRANCE' --epoch 1
#  python main.py --wandb --dataset Germany_processed_0 --wandb_project 'StemGNN_Germany'
#  python main.py --hyperparameter_search --dataset ECG_data --epoch 10

parser = argparse.ArgumentParser()
parser.add_argument('--train', type=bool, default=True)
parser.add_argument('--evaluate', type=bool, default=True)

# æ›´æ–°æ•°æ®é›†å‚æ•°ï¼Œæ·»åŠ æ‰€æœ‰å¯ç”¨æ•°æ®é›†çš„æ”¯æŒ
available_datasets = [
    'ECG_data', 'ECG_data_0', 'PeMS07', 
    'France_processed_0', 'Germany_processed_0'
]
parser.add_argument('--dataset', type=str, default='ECG_data', 
                   choices=available_datasets,
                   help=f'æ•°æ®é›†é€‰æ‹©ï¼Œå¯é€‰: {", ".join(available_datasets)}')

parser.add_argument('--show_datasets', action='store_true', help='æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨æ•°æ®é›†ä¿¡æ¯')

# è¶…å‚æ•°æœç´¢é€‰é¡¹
parser.add_argument('--hyperparameter_search', action='store_true', help='è¿›è¡Œwindow_sizeå’Œhorizonçš„è¶…å‚æ•°æœç´¢')

parser.add_argument('--window_size', type=int, default=96)
parser.add_argument('--horizon', type=int, default=96)
parser.add_argument('--train_length', type=float, default=7)
parser.add_argument('--valid_length', type=float, default=2)
parser.add_argument('--test_length', type=float, default=1)
parser.add_argument('--epoch', type=int, default=50)
parser.add_argument('--lr', type=float, default=1e-4)
parser.add_argument('--multi_layer', type=int, default=5)
parser.add_argument('--device', type=str, default='cuda:0')
parser.add_argument('--validate_freq', type=int, default=1)
parser.add_argument('--batch_size', type=int, default=32)
parser.add_argument('--norm_method', type=str, default='z_score')
parser.add_argument('--optimizer', type=str, default='RMSProp')
parser.add_argument('--early_stop', type=bool, default=False)
parser.add_argument('--exponential_decay_step', type=int, default=5)
parser.add_argument('--decay_rate', type=float, default=0.5)
parser.add_argument('--dropout_rate', type=float, default=0.5)
parser.add_argument('--leakyrelu_rate', type=int, default=0.2)


# æ–°å¢ wandb ç›¸å…³å‚æ•°
parser.add_argument('--runs', type=int, default=1, help='Number of runs to perform')
parser.add_argument('--wandb', action='store_true', help='Use wandb for experiment tracking')
parser.add_argument('--wandb_project', type=str, default='StemGNNECG', help='Wandb project name')
parser.add_argument('--experiment_name', type=str, default=None, help='Experiment name for organizing results')

args = parser.parse_args()

# å¦‚æœç”¨æˆ·è¦æ±‚æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯ï¼Œåˆ™æ˜¾ç¤ºå¹¶é€€å‡º
if args.show_datasets:
    print_dataset_info()
    exit(0)

# æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨
data_file = args.dataset + '.csv'
data_path = os.path.join('dataset', data_file)
if not os.path.exists(data_path):
    print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    print(f"å¯ç”¨çš„æ•°æ®é›†: {', '.join(available_datasets)}")
    print("\nä½¿ç”¨ --show_datasets æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")
    exit(1)

print(f"âœ“ ä½¿ç”¨æ•°æ®é›†: {args.dataset}")
print(f"âœ“ æ•°æ®é›†æ–‡ä»¶: {data_path}")

# è®¾ç½®å®éªŒåç§°
if args.experiment_name is None:
    if args.hyperparameter_search:
        args.experiment_name = f"{args.dataset}_hypersearch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    else:
        args.experiment_name = f"{args.dataset}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

print(f'Training configs: {args}')

# è¶…å‚æ•°æœç´¢å‡½æ•°
def hyperparameter_search():
    """æ‰§è¡Œè¶…å‚æ•°æœç´¢"""
    print("\n" + "="*60)
    print("å¼€å§‹è¶…å‚æ•°æœç´¢: window_size å’Œ horizon")
    print("="*60)
    
    # å®šä¹‰æœç´¢ç©ºé—´
    search_space = [6, 12, 48]
    results = []
    
    # ä¸ºæ¯ä¸ªwindow_sizeç”Ÿæˆæœ‰æ•ˆçš„horizonç»„åˆ
    valid_combinations = []
    for window_size in search_space:
        for horizon in search_space:
            if window_size >= horizon:  # window_sizeä¸èƒ½æ¯”horizonå°
                valid_combinations.append((window_size, horizon))
    
    print(f"æœ‰æ•ˆçš„å‚æ•°ç»„åˆ: {valid_combinations}")
    print(f"æ€»å…±è¦æµ‹è¯• {len(valid_combinations)} ç§ç»„åˆ\n")
    
    # åˆå§‹åŒ– wandbï¼ˆå¦‚æœå¯ç”¨ï¼‰
    wandb_run = None
    if args.wandb:
        try:
            import wandb
            wandb_run = wandb.init(
                project=args.wandb_project + "_hypersearch",
                name=args.experiment_name,
                config=vars(args),
                tags=[args.dataset, "hyperparameter_search"]
            )
            print("âœ“ Wandb è¶…å‚æ•°æœç´¢åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            print("âš  Wandb æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install wandb")
            args.wandb = False
        except Exception as e:
            print(f"âš  Wandb åˆå§‹åŒ–å¤±è´¥: {e}")
            args.wandb = False
    
    # éå†æ¯ä¸ªç»„åˆ
    for i, (window_size, horizon) in enumerate(valid_combinations):
        print(f"\n[{i+1}/{len(valid_combinations)}] æµ‹è¯• window_size={window_size}, horizon={horizon}")
        print("-" * 40)
        
        # æ›´æ–°argsä¸­çš„å‚æ•°
        args.window_size = window_size
        args.horizon = horizon
        
        try:
            # é‡æ–°åŠ è½½æ•°æ®é›†ï¼ˆå› ä¸ºwindow_sizeå’Œhorizonå˜äº†ï¼‰
            dataset_result = load_and_process_dataset(
                root_path='./dataset',
                data_file=data_file,
                target_column=None,
                features='M',
                seq_len=args.window_size,
                label_len=args.window_size // 2,
                pred_len=args.horizon,
                scale_to_01=True,
                batch_size=args.batch_size,
                freq='h',
                timeenc=0
            )
            
            # æå–æ•°æ®
            train_dataset = dataset_result['train_dataset']
            val_dataset = dataset_result['val_dataset'] 
            test_dataset = dataset_result['test_dataset']
            
            train_data = train_dataset.data_x
            valid_data = val_dataset.data_x
            test_data = test_dataset.data_x
            
            print(f"æ•°æ®åŠ è½½å®Œæˆ: è®­ç»ƒé›†{train_data.shape}, éªŒè¯é›†{valid_data.shape}, æµ‹è¯•é›†{test_data.shape}")
            
            # è®¾ç½®ç»“æœç›®å½•
            result_train_file = os.path.join('output', 'hypersearch', args.dataset, f'ws{window_size}_hz{horizon}', 'train')
            result_test_file = os.path.join('output', 'hypersearch', args.dataset, f'ws{window_size}_hz{horizon}', 'test')
            
            if not os.path.exists(result_train_file):
                os.makedirs(result_train_file)
            if not os.path.exists(result_test_file):
                os.makedirs(result_test_file)
            
            # è®­ç»ƒ
            if args.train:
                print("å¼€å§‹è®­ç»ƒ...")
                train_start = datetime.now()
                train_metrics, normalize_statistic = train(train_data, valid_data, args, result_train_file, wandb_run)
                train_end = datetime.now()
                train_time = (train_end - train_start).total_seconds() / 60
                print(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {train_time:.2f} åˆ†é’Ÿ")
            
            # æµ‹è¯•
            if args.evaluate:
                print("å¼€å§‹æµ‹è¯•...")
                test_start = datetime.now()
                test_metrics = test(test_data, args, result_train_file, result_test_file, wandb_run)
                test_end = datetime.now()
                test_time = (test_end - test_start).total_seconds() / 60
                print(f"æµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {test_time:.2f} åˆ†é’Ÿ")
                
                # ä¿å­˜ç»“æœ
                result = {
                    'window_size': window_size,
                    'horizon': horizon,
                    'mae': test_metrics.get('mae', 0),
                    'rmse': test_metrics.get('rmse', 0),
                    'mape': test_metrics.get('mape', 0),
                    'train_time_min': round(train_time, 2),
                    'test_time_min': round(test_time, 2)
                }
                results.append(result)
                
                # æ‰“å°å½“å‰ç»“æœ
                print(f"Seq Length {window_size:2d}, Pred Length {horizon:2d}: MAE={result['mae']:.4f}, RMSE={result['rmse']:.4f}")
                
                # è®°å½•åˆ°wandb
                if wandb_run:
                    wandb_run.log({
                        "window_size": window_size,
                        "horizon": horizon,
                        "test_mae": result['mae'],
                        "test_rmse": result['rmse'],
                        "test_mape": result['mape'],
                        "train_time_min": result['train_time_min'],
                        "test_time_min": result['test_time_min'],
                        "combination_id": i
                    })
        
        except Exception as e:
            print(f"âŒ å‚æ•°ç»„åˆ (window_size={window_size}, horizon={horizon}) å¤±è´¥: {e}")
            continue
    
    # ä¿å­˜å’Œæ˜¾ç¤ºæ‰€æœ‰ç»“æœ
    print("\n" + "="*60)
    print("è¶…å‚æ•°æœç´¢ç»“æœæ±‡æ€»")
    print("="*60)
    
    if results:
        # æŒ‰window_sizeåˆ†ç»„å¹¶æ˜¾ç¤ºç»“æœ
        results_by_window = {}
        for result in results:
            ws = result['window_size']
            if ws not in results_by_window:
                results_by_window[ws] = []
            results_by_window[ws].append(result)
        
        # æ˜¾ç¤ºæ ¼å¼åŒ–ç»“æœ
        all_results_text = []
        for window_size in sorted(results_by_window.keys()):
            window_results = results_by_window[window_size]
            # æŒ‰MAEæ’åºï¼Œæ‰¾å‡ºæœ€å¥½çš„horizon
            window_results.sort(key=lambda x: x['mae'])
            
            print(f"\nWindow Size {window_size} çš„ç»“æœ:")
            for result in window_results:
                result_text = f"Seq Length {result['window_size']:2d}, Pred Length {result['horizon']:2d}: MAE={result['mae']:.4f}, RMSE={result['rmse']:.4f}"
                print(f"  {result_text}")
                all_results_text.append(result_text)
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        results_file = os.path.join('output', f'hyperparameter_search_results_{args.dataset}.json')
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        # ä¿å­˜æ ¼å¼åŒ–ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶
        formatted_results_file = os.path.join('output', f'hyperparameter_search_summary_{args.dataset}.txt')
        with open(formatted_results_file, 'w') as f:
            f.write("è¶…å‚æ•°æœç´¢ç»“æœæ±‡æ€»\n")
            f.write("="*40 + "\n\n")
            for result_text in all_results_text:
                f.write(result_text + "\n")
        
        print(f"\nâœ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        print(f"âœ“ æ ¼å¼åŒ–ç»“æœå·²ä¿å­˜åˆ°: {formatted_results_file}")
        
        # æ‰¾å‡ºæœ€å¥½çš„ç»“æœ
        best_result = min(results, key=lambda x: x['mae'])
        print(f"\nğŸ† æœ€ä½³ç»“æœ:")
        print(f"Seq Length {best_result['window_size']:2d}, Pred Length {best_result['horizon']:2d}: MAE={best_result['mae']:.4f}, RMSE={best_result['rmse']:.4f}")
        
        if wandb_run:
            wandb_run.log({
                "best_window_size": best_result['window_size'],
                "best_horizon": best_result['horizon'], 
                "best_mae": best_result['mae'],
                "best_rmse": best_result['rmse'],
                "total_combinations_tested": len(results)
            })
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„ç»“æœ")
    
    # å…³é—­wandb
    if wandb_run:
        wandb.finish()
    
    return results

# ä¸»ç¨‹åºé€»è¾‘ä¿®æ”¹
if args.hyperparameter_search:
    # æ‰§è¡Œè¶…å‚æ•°æœç´¢
    hyperparameter_search()
    exit(0)

# åŸæœ‰çš„å•æ¬¡å®éªŒé€»è¾‘ä¿æŒä¸å˜
# åˆå§‹åŒ– wandbï¼ˆå¦‚æœå¯ç”¨ï¼‰
wandb_run = None
if args.wandb:
    try:
        import wandb
        wandb_run = wandb.init(
            project=args.wandb_project,
            name=args.experiment_name,
            config=vars(args),
            tags=[args.dataset]
        )
        print("âœ“ Wandb åˆå§‹åŒ–æˆåŠŸ")
    except ImportError:
        print("âš  Wandb æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install wandb")
        args.wandb = False
    except Exception as e:
        print(f"âš  Wandb åˆå§‹åŒ–å¤±è´¥: {e}")
        args.wandb = False

# åˆ›å»ºå®éªŒç›®å½•
if args.runs > 1:
    experiment_base_dir = os.path.join('output', args.experiment_name)
    result_train_file = os.path.join(experiment_base_dir, 'train')
    result_test_file = os.path.join(experiment_base_dir, 'test')
else:
    result_train_file = os.path.join('output', args.dataset, 'train')
    result_test_file = os.path.join('output', args.dataset, 'test')

if not os.path.exists(result_train_file):
    os.makedirs(result_train_file)
if not os.path.exists(result_test_file):
    os.makedirs(result_test_file)

# ä¿å­˜å®éªŒé…ç½®
if args.runs > 1:
    config_file = os.path.join(os.path.dirname(result_train_file), 'config.json')
    with open(config_file, 'w') as f:
        json.dump(vars(args), f, indent=2)

# ä½¿ç”¨ Dataset_Opennem ç±»åŠ è½½æ•°æ®é›†
print("\n" + "="*50)
print("å¼€å§‹ä½¿ç”¨Dataset_Opennemç±»åŠ è½½æ•°æ®é›†...")
print("="*50)
try:
    # ä½¿ç”¨ load_and_process_dataset å‡½æ•°ï¼Œå†…éƒ¨ä½¿ç”¨ CompatibleDataset_Opennem ç±»
    dataset_result = load_and_process_dataset(
        root_path='./dataset',
        data_file=data_file,
        target_column=None,  # è‡ªåŠ¨æ£€æµ‹ç›®æ ‡åˆ—
        features='M',        # å¤šå˜é‡é¢„æµ‹
        seq_len=args.window_size,
        label_len=args.window_size // 2,
        pred_len=args.horizon,
        scale_to_01=True,    # ç¡®ä¿æ•°æ®åœ¨0-1èŒƒå›´
        batch_size=args.batch_size,
        freq='h',
        timeenc=0
    )
    
    print("âœ“ Dataset_Opennem æ•°æ®é›†åŠ è½½æˆåŠŸ!")
    print(f"  æ•°æ®é›†åç§°: {dataset_result['dataset_name']}")
    print(f"  ç›®æ ‡åˆ—: {dataset_result['target_column']}")
    print(f"  ç‰¹å¾ç»´åº¦: {dataset_result['feature_dim']}")
    print(f"  æ€»æ ·æœ¬æ•°: {dataset_result['total_samples']}")
    print(f"  æ•°æ®å½¢çŠ¶: {dataset_result['data_shape']}")
    
    # ä»Dataset_Opennemå®ä¾‹ä¸­æå–numpyæ•°ç»„æ•°æ®
    # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦å°†æ•°æ®è½¬æ¢ä¸ºmain.pyçš„train/testå‡½æ•°æœŸæœ›çš„æ ¼å¼
    train_dataset = dataset_result['train_dataset']
    val_dataset = dataset_result['val_dataset'] 
    test_dataset = dataset_result['test_dataset']
    
    # æå–åŸå§‹æ•°æ®ï¼ˆå·²ç»æ˜¯0-1èŒƒå›´ï¼‰
    print("\næå–è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®...")
    train_data = train_dataset.data_x  # å·²ç»æ˜¯numpyæ•°ç»„ä¸”åœ¨0-1èŒƒå›´
    valid_data = val_dataset.data_x
    test_data = test_dataset.data_x
    
    print(f"âœ“ æ•°æ®æå–å®Œæˆ:")
    print(f"  è®­ç»ƒé›†å½¢çŠ¶: {train_data.shape}")
    print(f"  éªŒè¯é›†å½¢çŠ¶: {valid_data.shape}")
    print(f"  æµ‹è¯•é›†å½¢çŠ¶: {test_data.shape}")
    print(f"  æ•°æ®èŒƒå›´: [{train_data.min():.6f}, {train_data.max():.6f}]")
    print(f"  æ³¨æ„: Dataset_Opennemå·²è‡ªåŠ¨æŒ‰7:2:1æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†")
    
except Exception as e:
    print(f"âŒ Dataset_Opennem æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    exit(1)

# Dataset_Opennem å·²ç»å®Œæˆäº†æ•°æ®åˆ’åˆ†ï¼Œæ— éœ€æ‰‹åŠ¨åˆ’åˆ†
# split data
# train_ratio = args.train_length / (args.train_length + args.valid_length + args.test_length)
# valid_ratio = args.valid_length / (args.train_length + args.valid_length + args.test_length)
# test_ratio = 1 - train_ratio - valid_ratio
# train_data = train_data[:int(train_ratio * len(train_data))]
# valid_data = valid_data[int(train_ratio * len(valid_data)):int((train_ratio + valid_ratio) * len(valid_data))]
# test_data = test_data[int((train_ratio + valid_ratio) * len(test_data)):]

def save_results_to_csv(results_list, dataset_name, num_runs):
    """å°†è¿è¡Œç»“æœä¿å­˜åˆ°CSVæ–‡ä»¶"""
    
    # åˆ›å»ºæ–‡ä»¶å
    if num_runs == 1:
        filename = f"results_{dataset_name}_single_run.csv"
    else:
        filename = f"results_{dataset_name}_{num_runs}runs.csv"
    
    filepath = os.path.join('output', filename)
    
    # ç¡®ä¿outputç›®å½•å­˜åœ¨
    os.makedirs('output', exist_ok=True)
    
    # å®šä¹‰CSVåˆ—å
    fieldnames = [
        'run_id',
        'dataset',
        'timestamp',
        'final_test_mape',
        'final_test_mae', 
        'final_test_rmse',
        'total_train_time_min',
        'total_eval_time_min',
        'best_epoch',
        'total_params',
        'train_epochs',
        'batch_size',
        'learning_rate',
        'device',
        'window_size',
        'horizon'
    ]
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    file_exists = os.path.isfile(filepath)
    
    with open(filepath, 'a', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå†™å…¥è¡¨å¤´
        if not file_exists:
            writer.writeheader()
        
        # å†™å…¥æ¯æ¬¡è¿è¡Œçš„ç»“æœï¼Œåªä¿ç•™fieldnamesä¸­çš„å­—æ®µ
        for result in results_list:
            # è¿‡æ»¤æ‰ä¸åœ¨fieldnamesä¸­çš„å­—æ®µ
            filtered_result = {key: value for key, value in result.items() if key in fieldnames}
            writer.writerow(filtered_result)
    
    print(f"âœ“ è¿è¡Œç»“æœå·²ä¿å­˜åˆ°: {filepath}")
    return filepath

def run_single_experiment(run_id=0):
    """æ‰§è¡Œå•æ¬¡å®éªŒ"""
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(run_id)
    np.random.seed(run_id)
    
    print(f"å¼€å§‹ç¬¬ {run_id + 1}/{args.runs} æ¬¡è¿è¡Œ")
    
    results = {
        'run_id': run_id,
        'dataset': args.dataset,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'train_epochs': args.epoch,
        'batch_size': args.batch_size,
        'learning_rate': args.lr,
        'device': args.device,
        'window_size': args.window_size,
        'horizon': args.horizon
    }
    
    if args.train:
        try:
            before_train = datetime.now().timestamp()
            train_metrics, normalize_statistic = train(train_data, valid_data, args, result_train_file, wandb_run)
            after_train = datetime.now().timestamp()
            train_time = (after_train - before_train) / 60
            print(f'ç¬¬ {run_id + 1} æ¬¡è¿è¡Œè®­ç»ƒè€—æ—¶: {train_time:.2f} åˆ†é’Ÿ')
            
            results['total_train_time_min'] = round(train_time, 4)
            results['train_metrics'] = train_metrics
            
            # ä»è®­ç»ƒæŒ‡æ ‡ä¸­æå–ä¿¡æ¯
            if 'best_epoch' in train_metrics:
                results['best_epoch'] = train_metrics['best_epoch']
            if 'total_params' in train_metrics:
                results['total_params'] = train_metrics['total_params']
            
        except KeyboardInterrupt:
            print('-' * 99)
            print('æå‰é€€å‡ºè®­ç»ƒ')
            if wandb_run:
                wandb_run.log({"interrupted": True})
            results['interrupted'] = True
    
    if args.evaluate:
        before_evaluation = datetime.now().timestamp()
        test_metrics = test(test_data, args, result_train_file, result_test_file, wandb_run)
        after_evaluation = datetime.now().timestamp()
        eval_time = (after_evaluation - before_evaluation) / 60
        print(f'ç¬¬ {run_id + 1} æ¬¡è¿è¡Œè¯„ä¼°è€—æ—¶: {eval_time:.2f} åˆ†é’Ÿ')
        
        results['total_eval_time_min'] = round(eval_time, 4)
        results['test_metrics'] = test_metrics
        
        # æå–æµ‹è¯•æŒ‡æ ‡
        if isinstance(test_metrics, dict):
            results['final_test_mape'] = round(test_metrics.get('mape', 0), 6)
            results['final_test_mae'] = round(test_metrics.get('mae', 0), 6)
            results['final_test_rmse'] = round(test_metrics.get('rmse', 0), 6)
        
        # è®°å½•æœ€ç»ˆç»“æœåˆ° wandbï¼Œå¢åŠ æ›´å¤šè¯¦ç»†ä¿¡æ¯
        if wandb_run:
            log_data = {
                "final_test_mape": results.get('final_test_mape', 0),
                "final_test_mae": results.get('final_test_mae', 0),
                "final_test_rmse": results.get('final_test_rmse', 0),
                "total_train_time_min": results.get('total_train_time_min', 0),
                "total_eval_time_min": results.get('total_eval_time_min', 0),
                "run_id": run_id,
                "dataset_name": args.dataset,
                "experiment_name": args.experiment_name
            }
            
            # å¦‚æœæœ‰èŠ‚ç‚¹çº§åˆ«çš„æŒ‡æ ‡ï¼Œä¹Ÿè®°å½•
            if 'mae_node' in test_metrics:
                log_data.update({
                    "final_test_mae_node_count": len(test_metrics['mae_node']),
                    "final_test_mae_node_worst": np.max(test_metrics['mae_node']),
                    "final_test_mae_node_best": np.min(test_metrics['mae_node']),
                    "final_test_mape_node_worst": np.max(test_metrics['mape_node']),
                    "final_test_mape_node_best": np.min(test_metrics['mape_node']),
                    "final_test_rmse_node_worst": np.max(test_metrics['rmse_node']),
                    "final_test_rmse_node_best": np.min(test_metrics['rmse_node']),
                })
            
            wandb_run.log(log_data)
    
    return results

if __name__ == '__main__':
    if args.runs == 1:
        # å•æ¬¡è¿è¡Œ
        torch.manual_seed(0)
        results = run_single_experiment(0)
        
        # ä¿å­˜å•æ¬¡è¿è¡Œç»“æœåˆ°CSV
        save_results_to_csv([results], args.dataset, 1)
        
        print('done')
    else:
        # å¤šæ¬¡è¿è¡Œ
        print(f"å¼€å§‹ {args.runs} æ¬¡è¿è¡Œçš„å®éªŒ")
        all_results = []
        csv_results = []  # ç”¨äºä¿å­˜åˆ°CSVçš„ç»“æœåˆ—è¡¨
        
        for run_id in range(args.runs):
            result = run_single_experiment(run_id)
            all_results.append(result)
            csv_results.append(result)  # æ·»åŠ åˆ°CSVç»“æœåˆ—è¡¨
        
        # ä¿å­˜æ‰€æœ‰è¿è¡Œç»“æœåˆ°CSV
        save_results_to_csv(csv_results, args.dataset, args.runs)
        
        # è®¡ç®—å¤šæ¬¡è¿è¡Œçš„ç»Ÿè®¡ä¿¡æ¯
        if len(all_results) > 1:
            test_mapes = [r.get('final_test_mape', 0) for r in all_results if 'final_test_mape' in r and not r.get('interrupted', False)]
            test_maes = [r.get('final_test_mae', 0) for r in all_results if 'final_test_mae' in r and not r.get('interrupted', False)]
            test_rmses = [r.get('final_test_rmse', 0) for r in all_results if 'final_test_rmse' in r and not r.get('interrupted', False)]
            
            if test_mapes:
                print(f"\nå¤šæ¬¡è¿è¡Œç»“æœç»Ÿè®¡ ({len(test_mapes)} æ¬¡æˆåŠŸè¿è¡Œ):")
                print("-" * 50)
                print(f"MAPE: {np.mean(test_mapes):.4f} Â± {np.std(test_mapes):.4f}")
                print(f"MAE:  {np.mean(test_maes):.4f} Â± {np.std(test_maes):.4f}")
                print(f"RMSE: {np.mean(test_rmses):.4f} Â± {np.std(test_rmses):.4f}")
                
                # è®°å½•æ±‡æ€»ç»Ÿè®¡åˆ° wandb
                if wandb_run:
                    wandb_run.log({
                        "summary_mape_mean": np.mean(test_mapes),
                        "summary_mape_std": np.std(test_mapes),
                        "summary_mae_mean": np.mean(test_maes),
                        "summary_mae_std": np.std(test_maes),
                        "summary_rmse_mean": np.mean(test_rmses),
                        "summary_rmse_std": np.std(test_rmses),
                        "successful_runs": len(test_mapes),
                        "total_runs": args.runs
                    })
                
                # ä¿å­˜æ±‡æ€»ç»“æœ
                summary_results = {
                    'mape': {'mean': np.mean(test_mapes), 'std': np.std(test_mapes), 'values': test_mapes},
                    'mae': {'mean': np.mean(test_maes), 'std': np.std(test_maes), 'values': test_maes},
                    'rmse': {'mean': np.mean(test_rmses), 'std': np.std(test_rmses), 'values': test_rmses},
                    'successful_runs': len(test_mapes),
                    'total_runs': args.runs
                }
                
                summary_file = os.path.join(os.path.dirname(result_train_file), 'experiment_summary.json')
                with open(summary_file, 'w') as f:
                    json.dump(summary_results, f, indent=2)
        
        print('å®éªŒå®Œæˆ!')
    
    # å…³é—­ wandb
    if wandb_run:
        wandb.finish()


